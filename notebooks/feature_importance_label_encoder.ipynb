{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn import linear_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(15,)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "np.array(list(train_df_x.columns)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1    18230\n0      854\nName: MULTIPLE_OFFENSE, dtype: int64\ntrain_df_x shape is : (19084, 15)\n1    18230\n0    18230\nName: MULTIPLE_OFFENSE, dtype: int64\n"
    }
   ],
   "source": [
    "#### TESTING THE IMBLEARN #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_fold = pd.read_csv(\"../input/train_label_endoing.csv\")\n",
    "\n",
    "# Let's tale into the first fold only\n",
    "train_df = df_fold[df_fold.kfold!=0]\n",
    "valid_df = df_fold[df_fold.kfold==0]\n",
    "\n",
    "# Check the imbalance in the data for now\n",
    "print(train_df.MULTIPLE_OFFENSE.value_counts())\n",
    "\n",
    "# print(train_df.columns)\n",
    "\n",
    "train_df_x = train_df.drop(['DATE','INCIDENT_ID','MULTIPLE_OFFENSE','kfold'],axis=1).reset_index(drop=True)\n",
    "print(f\"train_df_x shape is : {train_df_x.shape}\")\n",
    "train_df_y = train_df.MULTIPLE_OFFENSE\n",
    "# train_df_y = pd.DataFrame({'MULTIPLE_OFFENSE' : train_df['MULTIPLE_OFFENSE']})\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X,y = ros.fit_resample(train_df_x, train_df_y)\n",
    "\n",
    "train_df = pd.concat([X,y], axis=1)\n",
    "\n",
    "# Check the distribution now\n",
    "print(train_df.MULTIPLE_OFFENSE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(23856, 19)\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# When I ran the model, it was somewhat concerning as all the classifications have been of a single class. Therefore, we will have to look into it,\n",
    "\n",
    "# Let's analyse the folds that we made, perhaps there was something fishy in the Stratified K-Fold sampling that we have done\n",
    "df_fold = pd.read_csv(\"../input/train_folds.csv\")\n",
    "df_fold.head()\n",
    "print(df_fold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Current fold : 0\n1    4558\n0     214\nName: MULTIPLE_OFFENSE, dtype: int64\nCurrent fold : 1\n1    4558\n0     213\nName: MULTIPLE_OFFENSE, dtype: int64\nCurrent fold : 2\n1    4558\n0     213\nName: MULTIPLE_OFFENSE, dtype: int64\nCurrent fold : 3\n1    4557\n0     214\nName: MULTIPLE_OFFENSE, dtype: int64\nCurrent fold : 4\n1    4557\n0     214\nName: MULTIPLE_OFFENSE, dtype: int64\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Current fold : {i}\")\n",
    "    print(df_fold[df_fold['kfold']==i].MULTIPLE_OFFENSE.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore the stratified sampling technique looks on point, but three is a sever class imbalance, maybe we can look into some upsampling techniques to improve the quality of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/saransh/Programming/Hiring_Challenge_Novartis/Dataset/notebooks\n"
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model_name, model_type):\n",
    "    l = os.listdir(f\"../models/{model_name}/\")\n",
    "    all_clf = {}\n",
    "    for f in l:\n",
    "        if f.find(\".pkl\")!=-1:\n",
    "            name = f[:f.find(\".pkl\")]\n",
    "            all_clf[name] = joblib.load(f\"../models/{model_name}/{f}\")\n",
    "    feautre_importance = {}\n",
    "    for fold in range(5):\n",
    "        if model_type=='linear':\n",
    "            l = list(all_clf[f\"fold_{fold}\"].coef_[0])\n",
    "        elif model_type=='tree':\n",
    "            l = all_clf[f\"fold_{fold}\"].feature_importances_\n",
    "        else:\n",
    "            raise Exception(\"Not implemented\")\n",
    "        new_l = []\n",
    "        for (i,val) in enumerate(l):\n",
    "            curr_tup = (f\"X_{i+1}\",val)\n",
    "            new_l.append(curr_tup)\n",
    "        new_l.sort(key= lambda x : x[1], reverse=True)\n",
    "        feautre_importance[f\"fold_{fold}\"] = new_l\n",
    "    return feautre_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feautre_importance_1e_minus_6 = get_feature_importance(\"logistic_regression_c_1e-6_label_encoder\")\n",
    "feature_importance_decision_trees = get_feature_importance(\"decision_tree_label_encoder\", model_type=\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fold_0': [('X_10', 0.6235456405129233),\n  ('X_15', 0.18588508216124405),\n  ('X_11', 0.15978200543955579),\n  ('X_12', 0.01463285779593148),\n  ('X_4', 0.0040536165319473745),\n  ('X_6', 0.0030145988288831317),\n  ('X_2', 0.0022399821480747592),\n  ('X_7', 0.0019949824521285444),\n  ('X_8', 0.0009131786487371007),\n  ('X_9', 0.0008941079938577485),\n  ('X_14', 0.0008351919467216522),\n  ('X_3', 0.0008293042487241414),\n  ('X_13', 0.0007810565940686994),\n  ('X_5', 0.00038509128454306674),\n  ('X_1', 0.00021330341265914546)],\n 'fold_1': [('X_10', 0.6176375937226721),\n  ('X_11', 0.16152356891337977),\n  ('X_12', 0.12229115762271851),\n  ('X_15', 0.08657999982347803),\n  ('X_6', 0.003996913557694143),\n  ('X_4', 0.003722275086535396),\n  ('X_7', 0.0014204707101249637),\n  ('X_14', 0.0007910634035203823),\n  ('X_2', 0.0007535294627132834),\n  ('X_3', 0.0004435479926545746),\n  ('X_13', 0.00032090061933364675),\n  ('X_1', 0.0001935938134656933),\n  ('X_5', 0.0001113658741256809),\n  ('X_9', 0.00010744467773427283),\n  ('X_8', 0.00010657471984954157)],\n 'fold_2': [('X_10', 0.6003394622309047),\n  ('X_11', 0.1649419379138408),\n  ('X_12', 0.12465969129994796),\n  ('X_15', 0.0999897908261495),\n  ('X_7', 0.002825491646924645),\n  ('X_4', 0.0022846942185493456),\n  ('X_3', 0.0018252533742121615),\n  ('X_6', 0.0015830722423231997),\n  ('X_13', 0.0009068795437173064),\n  ('X_14', 0.0004192221361903588),\n  ('X_1', 0.0001080602951928363),\n  ('X_8', 0.00010513805083196205),\n  ('X_5', 1.1306221215254605e-05),\n  ('X_2', 0.0),\n  ('X_9', 0.0)],\n 'fold_3': [('X_10', 0.6059550527110852),\n  ('X_11', 0.15834561840250413),\n  ('X_12', 0.12438503001612133),\n  ('X_15', 0.09459451834352796),\n  ('X_7', 0.005293024534265223),\n  ('X_3', 0.003041168274103917),\n  ('X_14', 0.002380386071256582),\n  ('X_6', 0.0013515509871913553),\n  ('X_5', 0.001122469738403067),\n  ('X_8', 0.0009293551424578414),\n  ('X_4', 0.0009007849590231521),\n  ('X_2', 0.0007713795655452069),\n  ('X_13', 0.0005078821757439787),\n  ('X_9', 0.00021599974601795697),\n  ('X_1', 0.00020577933275316967)],\n 'fold_4': [('X_10', 0.6236567653623433),\n  ('X_15', 0.18671258518611286),\n  ('X_11', 0.16622117109967127),\n  ('X_12', 0.014462314100797508),\n  ('X_13', 0.0023602389174548137),\n  ('X_4', 0.0016617024804114011),\n  ('X_6', 0.0016477513686494177),\n  ('X_7', 0.0010409511820762365),\n  ('X_14', 0.0005175226978563549),\n  ('X_5', 0.0005151539118002011),\n  ('X_2', 0.0005019173374567452),\n  ('X_3', 0.00048404161064924113),\n  ('X_8', 0.00011269600980838707),\n  ('X_1', 0.00010518873491231289),\n  ('X_9', 0.0)]}"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "feature_importance_decision_trees"
   ]
  },
  {
   "source": [
    "It is a mutual consensus in the models that X_10 is the most relevant features are as follows: <br>\n",
    "1) X_10 <br>\n",
    "2) X_12 <br>\n",
    "3) X_3 <br>\n",
    "4) X_15 <br>\n",
    "5) X_8 <br>\n",
    "6) X_2 <br>\n",
    "7) X_14 <br>\n",
    "8) X_5 / X_9 <br>\n",
    "<p>\n",
    "Till the 7) point, the values the feature importance is in this very order"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feautre_importance_1e_2 = get_feature_importance(\"logistic_regression_c_1e2_label_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fold_0': [('X_10', -0.5467936023563545),\n  ('X_3', -0.07611808956108321),\n  ('X_11', 0.0005285659439529121),\n  ('X_7', 0.008510933100943127),\n  ('X_14', 0.008641345115653218),\n  ('X_13', 0.009105210133295058),\n  ('X_5', 0.013559110145976959),\n  ('X_8', 0.020228469722725538),\n  ('X_15', 0.035932648795291636),\n  ('X_4', 0.045476993648164044),\n  ('X_6', 0.06546583327889302),\n  ('X_2', 0.08299224078115794),\n  ('X_9', 0.24636510009066115),\n  ('X_1', 0.2816575845419582),\n  ('X_12', 0.3632192293853987)],\n 'fold_1': [('X_10', -0.5323255977142577),\n  ('X_3', -0.03895249695761881),\n  ('X_15', -0.00630728138518147),\n  ('X_11', -0.00015015887715144887),\n  ('X_14', 0.004642697473173597),\n  ('X_13', 0.006923216376830237),\n  ('X_7', 0.012106727035604621),\n  ('X_6', 0.02685737639192136),\n  ('X_8', 0.028383032221383053),\n  ('X_2', 0.028501570361788717),\n  ('X_5', 0.03843525383747897),\n  ('X_4', 0.045944411538130364),\n  ('X_9', 0.18797271945084204),\n  ('X_12', 0.35160693348307526),\n  ('X_1', 0.437186968750495)],\n 'fold_2': [('X_10', -0.5428267045012097),\n  ('X_3', -0.02502898695663125),\n  ('X_7', -0.001210872976907874),\n  ('X_11', -1.912450787999742e-05),\n  ('X_13', 0.006199320368677641),\n  ('X_14', 0.007736422792103327),\n  ('X_2', 0.0164950844439817),\n  ('X_8', 0.023999744325121815),\n  ('X_15', 0.03504827171875455),\n  ('X_9', 0.040753658251455306),\n  ('X_6', 0.05579343244083211),\n  ('X_5', 0.06067531126553183),\n  ('X_4', 0.07820977688709674),\n  ('X_12', 0.3752099202722277),\n  ('X_1', 0.46557868288608406)],\n 'fold_3': [('X_10', -0.5257499106794098),\n  ('X_3', -0.06709439837662673),\n  ('X_15', -0.011437739661460522),\n  ('X_9', -0.0065218681459198805),\n  ('X_11', -0.004741082207708896),\n  ('X_14', 1.9803358931240727e-05),\n  ('X_13', 0.000806852094857634),\n  ('X_7', 0.007625322318708303),\n  ('X_6', 0.01979901677856336),\n  ('X_8', 0.046576281353706867),\n  ('X_2', 0.050980496110405685),\n  ('X_5', 0.063855187616169),\n  ('X_4', 0.0714074254621015),\n  ('X_1', 0.15288447982501963),\n  ('X_12', 0.33754836294577034)],\n 'fold_4': [('X_10', -0.5265114942335524),\n  ('X_3', -0.08489937120873761),\n  ('X_11', -9.130660976983395e-05),\n  ('X_13', 0.004897696477488665),\n  ('X_5', 0.006100069322014383),\n  ('X_14', 0.00876341100698909),\n  ('X_7', 0.00910329625283067),\n  ('X_8', 0.015943059181104904),\n  ('X_15', 0.028613516522247997),\n  ('X_6', 0.05834681597115294),\n  ('X_4', 0.06746112044940578),\n  ('X_2', 0.08907682568103152),\n  ('X_9', 0.08924958321731487),\n  ('X_12', 0.3460122209984091),\n  ('X_1', 0.47428677157046645)],\n 'fold_5': [('X_10', -0.5053774250568728),\n  ('X_3', -0.029634101682893422),\n  ('X_11', -0.0008176442966286564),\n  ('X_13', 0.0035466860128991976),\n  ('X_7', 0.00837931715584449),\n  ('X_14', 0.009287274523894908),\n  ('X_8', 0.010437846826709316),\n  ('X_15', 0.028425038320817157),\n  ('X_9', 0.03109828043338154),\n  ('X_2', 0.03232869808123383),\n  ('X_5', 0.0414009579319294),\n  ('X_4', 0.05267229436251955),\n  ('X_6', 0.061016871576789526),\n  ('X_1', 0.2512958290059212),\n  ('X_12', 0.33010255850333137)],\n 'fold_6': [('X_10', -0.5176763218639211),\n  ('X_3', -0.052111694899579855),\n  ('X_9', -0.03756952121911133),\n  ('X_7', -0.016319796571812553),\n  ('X_11', -0.002933614659661911),\n  ('X_14', 0.0016147059359465296),\n  ('X_13', 0.005423830587382432),\n  ('X_8', 0.01554380454717133),\n  ('X_15', 0.020349281571979384),\n  ('X_2', 0.03845239065100856),\n  ('X_6', 0.07073656762847916),\n  ('X_4', 0.07910733322284132),\n  ('X_5', 0.08531773831725131),\n  ('X_1', 0.32036486700383815),\n  ('X_12', 0.3382422651571093)],\n 'fold_7': [('X_10', -0.5441593520772242),\n  ('X_3', -0.07930282892196547),\n  ('X_7', -0.015157278397272229),\n  ('X_11', -2.9660475736155084e-05),\n  ('X_13', 0.007681579860802619),\n  ('X_14', 0.00773504816992981),\n  ('X_5', 0.012112197593827822),\n  ('X_8', 0.017280905569474068),\n  ('X_15', 0.024068056443951272),\n  ('X_4', 0.0543164281242697),\n  ('X_2', 0.08293054297441126),\n  ('X_6', 0.08824517032083017),\n  ('X_9', 0.19636440331925137),\n  ('X_12', 0.3663129075330308),\n  ('X_1', 0.3990652450976373)],\n 'fold_8': [('X_10', -0.5382493622299623),\n  ('X_3', -0.043480674648945006),\n  ('X_11', 0.0009530243017156854),\n  ('X_13', 0.006472764498811935),\n  ('X_14', 0.007775727820878455),\n  ('X_7', 0.014471334183322191),\n  ('X_8', 0.01973694758314295),\n  ('X_15', 0.0288599480670969),\n  ('X_5', 0.031498998857033854),\n  ('X_4', 0.03824847991322346),\n  ('X_2', 0.044569719904897415),\n  ('X_6', 0.047058586128706434),\n  ('X_9', 0.2141783379692158),\n  ('X_1', 0.3436278046268345),\n  ('X_12', 0.358163109783337)],\n 'fold_9': [('X_10', -0.5028174219780871),\n  ('X_3', -0.07909914911971638),\n  ('X_11', -0.0013079853318031917),\n  ('X_14', 0.006824637055650493),\n  ('X_5', 0.007207181265101672),\n  ('X_13', 0.0076742775122334),\n  ('X_8', 0.01105091079098023),\n  ('X_7', 0.018153415024727972),\n  ('X_15', 0.02484206041641416),\n  ('X_6', 0.044371785042667464),\n  ('X_4', 0.07082165630824154),\n  ('X_2', 0.07869898745074026),\n  ('X_9', 0.13543549906209645),\n  ('X_12', 0.33044920303158926),\n  ('X_1', 0.4434365997943312)]}"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "feautre_importance_1e_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}